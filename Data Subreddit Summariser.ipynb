{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4dfd06a",
   "metadata": {},
   "source": [
    "# Background\n",
    "I am a follower in the r/dataanalysis and r/datascience subbredits. I have always found the adundance of advice and ideas on there to be very useful. However, I came across a problem where I just did not have the time and patience to go thoroughly read hundreds of posts.\n",
    "\n",
    "Hence, I am creating a python script which extracts posts from r/dataanalysis and r/datascience, which then cleans and returns only the most useful summarised information for me.\n",
    "\n",
    "I have used PRAW package (Python Reddit API Wrapper) to retreive ~300 of the top and hottest posts from each of these subreddits. In addition, I have also extracted the top 10 comments from each of these posts.\n",
    "\n",
    "In this notebook, I will be cleaning and preparing the data for an interactive app which achieves my goal:\n",
    "\n",
    "#### Gather all the given career advice into one easily digestable place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5732e0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import contractions # contractions\n",
    "from nltk.corpus import stopwords # stopwords\n",
    "from nltk.stem import WordNetLemmatizer # lemmatiser\n",
    "import yake # keyword extraction\n",
    "import heapq\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "644e6cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author_name</th>\n",
       "      <th>created_unix</th>\n",
       "      <th>flair</th>\n",
       "      <th>score</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataanalysis</td>\n",
       "      <td>wfgn7j</td>\n",
       "      <td>After 2 months, 150 resumes, 6 interviews, I f...</td>\n",
       "      <td>SomeEmotion3</td>\n",
       "      <td>1.659557e+09</td>\n",
       "      <td>Career Advice</td>\n",
       "      <td>469</td>\n",
       "      <td>0.99</td>\n",
       "      <td>The job is Data Analyst for a well known Analy...</td>\n",
       "      <td>https://www.reddit.com/r/dataanalysis/comments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataanalysis</td>\n",
       "      <td>10onhl2</td>\n",
       "      <td>Want to become an analyst? Start here.</td>\n",
       "      <td>milwted</td>\n",
       "      <td>1.675038e+09</td>\n",
       "      <td>Career Advice</td>\n",
       "      <td>459</td>\n",
       "      <td>0.99</td>\n",
       "      <td>Starting a career in data analytics can open u...</td>\n",
       "      <td>https://www.reddit.com/r/dataanalysis/comments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataanalysis</td>\n",
       "      <td>unoys0</td>\n",
       "      <td>Google Apprenticeship Response from Google 2022</td>\n",
       "      <td>Danielle-Dee</td>\n",
       "      <td>1.652318e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>444</td>\n",
       "      <td>0.99</td>\n",
       "      <td>I applied for the Google Apprenticeship and I ...</td>\n",
       "      <td>https://www.reddit.com/r/dataanalysis/comments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataanalysis</td>\n",
       "      <td>z0mrku</td>\n",
       "      <td>SQL roadmap, things you should know</td>\n",
       "      <td>JamySun</td>\n",
       "      <td>1.668997e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>437</td>\n",
       "      <td>0.99</td>\n",
       "      <td>Most important SQL command and function, hope ...</td>\n",
       "      <td>https://i.redd.it/qpoytkb0a91a1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataanalysis</td>\n",
       "      <td>z1v48z</td>\n",
       "      <td>It really be like that</td>\n",
       "      <td>toketoornot</td>\n",
       "      <td>1.669128e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>371</td>\n",
       "      <td>0.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://i.redd.it/o234ckej1k1a1.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      subreddit       id                                              title  \\\n",
       "0  dataanalysis   wfgn7j  After 2 months, 150 resumes, 6 interviews, I f...   \n",
       "1  dataanalysis  10onhl2             Want to become an analyst? Start here.   \n",
       "2  dataanalysis   unoys0    Google Apprenticeship Response from Google 2022   \n",
       "3  dataanalysis   z0mrku                SQL roadmap, things you should know   \n",
       "4  dataanalysis   z1v48z                             It really be like that   \n",
       "\n",
       "    author_name  created_unix          flair  score  upvote_ratio  \\\n",
       "0  SomeEmotion3  1.659557e+09  Career Advice    469          0.99   \n",
       "1       milwted  1.675038e+09  Career Advice    459          0.99   \n",
       "2  Danielle-Dee  1.652318e+09            NaN    444          0.99   \n",
       "3       JamySun  1.668997e+09            NaN    437          0.99   \n",
       "4   toketoornot  1.669128e+09            NaN    371          0.99   \n",
       "\n",
       "                                         description  \\\n",
       "0  The job is Data Analyst for a well known Analy...   \n",
       "1  Starting a career in data analytics can open u...   \n",
       "2  I applied for the Google Apprenticeship and I ...   \n",
       "3  Most important SQL command and function, hope ...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.reddit.com/r/dataanalysis/comments...  \n",
       "1  https://www.reddit.com/r/dataanalysis/comments...  \n",
       "2  https://www.reddit.com/r/dataanalysis/comments...  \n",
       "3                https://i.redd.it/qpoytkb0a91a1.jpg  \n",
       "4                https://i.redd.it/o234ckej1k1a1.jpg  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top = pd.read_csv(\".\\\\data\\\\top_data_posts.csv\")\n",
    "top_comments = pd.read_csv(\".\\\\data\\\\top_data_post_comments.csv\")\n",
    "top.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db71d9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author_name</th>\n",
       "      <th>created_unix</th>\n",
       "      <th>flair</th>\n",
       "      <th>score</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataanalysis</td>\n",
       "      <td>11dl3sf</td>\n",
       "      <td>Hoping this video helps you in your data analy...</td>\n",
       "      <td>sujaynadkarni</td>\n",
       "      <td>1.677528e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14</td>\n",
       "      <td>0.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://youtu.be/P7OTI17Wp-M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataanalysis</td>\n",
       "      <td>11dpdoa</td>\n",
       "      <td>Very good data analytics article on HBR</td>\n",
       "      <td>ozarzoso</td>\n",
       "      <td>1.677538e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>0.81</td>\n",
       "      <td>Hello. I strongly recommend you all this outst...</td>\n",
       "      <td>https://www.reddit.com/r/dataanalysis/comments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataanalysis</td>\n",
       "      <td>11dgh1t</td>\n",
       "      <td>urgent: job only requires Google analytics</td>\n",
       "      <td>RaceyDesiWithNoFacey</td>\n",
       "      <td>1.677517e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14</td>\n",
       "      <td>0.77</td>\n",
       "      <td>I'm a fresher and trying to break into the fie...</td>\n",
       "      <td>https://www.reddit.com/r/dataanalysis/comments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataanalysis</td>\n",
       "      <td>11dswih</td>\n",
       "      <td>Math Teacher to Data Specialist</td>\n",
       "      <td>Sea_Obligation_2802</td>\n",
       "      <td>1.677547e+09</td>\n",
       "      <td>Career Advice</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>Hello, I am looking to get out of teaching. I ...</td>\n",
       "      <td>https://www.reddit.com/r/dataanalysis/comments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataanalysis</td>\n",
       "      <td>11czods</td>\n",
       "      <td>Data analysts who make 120k+ per year - what s...</td>\n",
       "      <td>garbage_gemlin</td>\n",
       "      <td>1.677463e+09</td>\n",
       "      <td>Career Advice</td>\n",
       "      <td>68</td>\n",
       "      <td>0.97</td>\n",
       "      <td>I am a data analyst and love my current job, b...</td>\n",
       "      <td>https://www.reddit.com/r/dataanalysis/comments...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      subreddit       id                                              title  \\\n",
       "0  dataanalysis  11dl3sf  Hoping this video helps you in your data analy...   \n",
       "1  dataanalysis  11dpdoa            Very good data analytics article on HBR   \n",
       "2  dataanalysis  11dgh1t         urgent: job only requires Google analytics   \n",
       "3  dataanalysis  11dswih                    Math Teacher to Data Specialist   \n",
       "4  dataanalysis  11czods  Data analysts who make 120k+ per year - what s...   \n",
       "\n",
       "            author_name  created_unix          flair  score  upvote_ratio  \\\n",
       "0         sujaynadkarni  1.677528e+09            NaN     14          0.95   \n",
       "1              ozarzoso  1.677538e+09            NaN      6          0.81   \n",
       "2  RaceyDesiWithNoFacey  1.677517e+09            NaN     14          0.77   \n",
       "3   Sea_Obligation_2802  1.677547e+09  Career Advice      2          0.75   \n",
       "4        garbage_gemlin  1.677463e+09  Career Advice     68          0.97   \n",
       "\n",
       "                                         description  \\\n",
       "0                                                NaN   \n",
       "1  Hello. I strongly recommend you all this outst...   \n",
       "2  I'm a fresher and trying to break into the fie...   \n",
       "3  Hello, I am looking to get out of teaching. I ...   \n",
       "4  I am a data analyst and love my current job, b...   \n",
       "\n",
       "                                                 url  \n",
       "0                       https://youtu.be/P7OTI17Wp-M  \n",
       "1  https://www.reddit.com/r/dataanalysis/comments...  \n",
       "2  https://www.reddit.com/r/dataanalysis/comments...  \n",
       "3  https://www.reddit.com/r/dataanalysis/comments...  \n",
       "4  https://www.reddit.com/r/dataanalysis/comments...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hot = pd.read_csv(\".\\\\data\\\\hot_data_posts.csv\")\n",
    "hot_comments = pd.read_csv(\".\\\\data\\\\hot_data_post_comments.csv\")\n",
    "hot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c862b8",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dd45b2",
   "metadata": {},
   "source": [
    "#### Keep only non-image (usually memes) posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e5851e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep non-image (usually memes) TOP posts\n",
    "non_meme_post_index = [i for i, url in enumerate(top['url']) if re.search(\".(jpg|png|gif)\", url) is None] # get non-image index   \n",
    "top = top.iloc[non_meme_post_index, :] # keep only non-image rows\n",
    "top_comments = top_comments[top_comments['post_id'].isin(top['id'])] # keep non-image posts\n",
    "# keep non-image (usually memes) HOT posts\n",
    "non_meme_post_index = [i for i, url in enumerate(hot['url']) if re.search(\".(jpg|png|gif)\", url) is None]    \n",
    "hot = hot.iloc[non_meme_post_index, :]\n",
    "hot_comments = hot_comments[hot_comments['post_id'].isin(hot['id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc54b2ae",
   "metadata": {},
   "source": [
    "#### Replace missing (NA) values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62b91745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 384 entries, 0 to 596\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   subreddit     384 non-null    object \n",
      " 1   id            384 non-null    object \n",
      " 2   title         384 non-null    object \n",
      " 3   author_name   366 non-null    object \n",
      " 4   created_unix  384 non-null    float64\n",
      " 5   flair         262 non-null    object \n",
      " 6   score         384 non-null    int64  \n",
      " 7   upvote_ratio  384 non-null    float64\n",
      " 8   description   322 non-null    object \n",
      " 9   url           384 non-null    object \n",
      "dtypes: float64(2), int64(1), object(7)\n",
      "memory usage: 33.0+ KB\n"
     ]
    }
   ],
   "source": [
    "top.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c88e35d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NA in author_name\n",
    "top['author_name'] = top['author_name'].fillna('DELETED ACCOUNT')\n",
    "hot['author_name'] = hot['author_name'].fillna('DELETED ACCOUNT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b3f8196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author_name</th>\n",
       "      <th>created_unix</th>\n",
       "      <th>flair</th>\n",
       "      <th>score</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>dataanalysis</td>\n",
       "      <td>k40wpj</td>\n",
       "      <td>Thought you guys might like this</td>\n",
       "      <td>SicDev</td>\n",
       "      <td>1.606759e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>142</td>\n",
       "      <td>0.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://gfycat.com/conventionalanchoredgardens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>dataanalysis</td>\n",
       "      <td>stxqf8</td>\n",
       "      <td>Animated Voronoi Diagram Showing Spacial Contr...</td>\n",
       "      <td>falseNaoi</td>\n",
       "      <td>1.645023e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>122</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://v.redd.it/zd31azeik7i81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>dataanalysis</td>\n",
       "      <td>vqgmhy</td>\n",
       "      <td>What are visualizations like this called? And ...</td>\n",
       "      <td>ChapliKebab</td>\n",
       "      <td>1.656852e+09</td>\n",
       "      <td>Data Question</td>\n",
       "      <td>108</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://v.redd.it/df6jannmkc991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>dataanalysis</td>\n",
       "      <td>wrghuu</td>\n",
       "      <td>data analyst interview questions: Link to the ...</td>\n",
       "      <td>matarrwolfenstein</td>\n",
       "      <td>1.660820e+09</td>\n",
       "      <td>Career Advice</td>\n",
       "      <td>102</td>\n",
       "      <td>0.94</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://media-exp1.licdn.com/dms/document/C4D1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>dataanalysis</td>\n",
       "      <td>zlmwkh</td>\n",
       "      <td>I've noticed a lot of posts about people tryin...</td>\n",
       "      <td>alorentz</td>\n",
       "      <td>1.671011e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94</td>\n",
       "      <td>0.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://whaly.io/posts/the-2023-data-analyst-s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       subreddit      id                                              title  \\\n",
       "34  dataanalysis  k40wpj                   Thought you guys might like this   \n",
       "45  dataanalysis  stxqf8  Animated Voronoi Diagram Showing Spacial Contr...   \n",
       "51  dataanalysis  vqgmhy  What are visualizations like this called? And ...   \n",
       "57  dataanalysis  wrghuu  data analyst interview questions: Link to the ...   \n",
       "62  dataanalysis  zlmwkh  I've noticed a lot of posts about people tryin...   \n",
       "\n",
       "          author_name  created_unix          flair  score  upvote_ratio  \\\n",
       "34             SicDev  1.606759e+09            NaN    142          0.99   \n",
       "45          falseNaoi  1.645023e+09            NaN    122          1.00   \n",
       "51        ChapliKebab  1.656852e+09  Data Question    108          1.00   \n",
       "57  matarrwolfenstein  1.660820e+09  Career Advice    102          0.94   \n",
       "62           alorentz  1.671011e+09            NaN     94          0.99   \n",
       "\n",
       "   description                                                url  \n",
       "34         NaN  https://gfycat.com/conventionalanchoredgardens...  \n",
       "45         NaN                    https://v.redd.it/zd31azeik7i81  \n",
       "51         NaN                    https://v.redd.it/df6jannmkc991  \n",
       "57         NaN  https://media-exp1.licdn.com/dms/document/C4D1...  \n",
       "62         NaN  https://whaly.io/posts/the-2023-data-analyst-s...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Investigate/Replace NA in description\n",
    "top[top['description'].isna()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc50142",
   "metadata": {},
   "source": [
    "Looking at URLs, these posts are referencing to other sources, and are often less with no description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8e8ff24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fun/Trivia                10\n",
       "Career Advice              9\n",
       "Tooling                    4\n",
       "Data Analysis Tutorial     3\n",
       "Discussion                 3\n",
       "Education                  3\n",
       "Data Question              2\n",
       "Resume Help                2\n",
       "Job Search                 2\n",
       "Projects                   2\n",
       "Employment Opportunity     1\n",
       "Project Feedback           1\n",
       "Networking                 1\n",
       "Name: flair, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top[top['description'].isna()]['flair'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f003e2",
   "metadata": {},
   "source": [
    "Looking at flairs of these missing posts, we can remove posts which are for fun/trivia. Other types of post may be useful to our goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d06ccf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Fun/Trivia flaired posts\n",
    "top = top[top['flair'] != 'Fun/Trivia']\n",
    "top_comments = top_comments[top_comments['post_id'].isin(top['id'])]\n",
    "hot = hot[hot['flair'] != 'Fun/Trivia']\n",
    "hot_comments = hot_comments[hot_comments['post_id'].isin(top['id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eacfb515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing description and flair with empty string\n",
    "top.loc[top['description'].isna(), 'description'] = \"\"\n",
    "hot.loc[hot['description'].isna(), 'description'] = \"\"\n",
    "\n",
    "top.loc[top['flair'].isna(), 'flair'] = \"\"\n",
    "hot.loc[hot['flair'].isna(), 'flair'] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29ea63e",
   "metadata": {},
   "source": [
    "#### Keep 'useful' posts\n",
    "Useful in our case means a score of more than 1. Posts by default on creation have a score of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b412f6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep useful posts (i.e. score > 1)\n",
    "top = top[top['score'] > 1]\n",
    "top_comments = top_comments[top_comments['score'] > 1]\n",
    "# keep useful posts (i.e. score > 1)\n",
    "hot = hot[hot['score'] > 1]\n",
    "hot_comments = hot_comments[hot_comments['score'] > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3558deae",
   "metadata": {},
   "source": [
    "#### Replace UNIX time with datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "388d5e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert unix time to date\n",
    "top['created_unix'] = [datetime.utcfromtimestamp(dt).strftime('%Y-%m-%d') for dt in top['created_unix']]\n",
    "hot['created_unix'] = [datetime.utcfromtimestamp(dt).strftime('%Y-%m-%d') for dt in hot['created_unix']]\n",
    "# rename column\n",
    "top = top.rename(columns={'created_unix': 'datetime'})\n",
    "hot = hot.rename(columns={'created_unix': 'datetime'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a69ff4",
   "metadata": {},
   "source": [
    "#### Clean text columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60af04da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(array): # expecting list or pandas series\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemma = WordNetLemmatizer()\n",
    "    new_desc = []\n",
    "    for text in array:\n",
    "        if text == text: # captures NaN\n",
    "            # 1. normalise (lowercase) text\n",
    "            text = text.lower()\n",
    "            # 2. expand formal contractions (e.g. i'll, haven't, don't, etc.)\n",
    "            text = \" \".join([contractions.fix(w) for w in text.split()])\n",
    "            # 3. remove unicode characters (note: don't remove digits)\n",
    "            text = re.sub(r\"https?://\\S+\", \"\", text) # remove urls\n",
    "            text = re.sub(r\"([^a-z0-9.])\", \" \", text) # keep only character, digit, or fullstop (for sentences)\n",
    "            text = re.sub(r\"\\s{2,}\", \" \", text) # replace multiple spaces with one space\n",
    "            # 4. remove stopwords\n",
    "            text = \" \".join([w for w in text.split() if w.strip(\".\") not in stop_words])\n",
    "            # 5. lemmatise each word (group words based on root/origin)\n",
    "            text = \" \".join([lemma.lemmatize(w) for w in text.split()])\n",
    "            new_desc.append(text)\n",
    "        else:\n",
    "            new_desc.append(\"\")\n",
    "    return new_desc\n",
    "\n",
    "top['description'] = clean_text(top['description'])\n",
    "hot['description'] = clean_text(hot['description'])\n",
    "top['title'] = clean_text(top['title'])\n",
    "hot['title'] = clean_text(hot['title'])\n",
    "top_comments['comment'] = clean_text(top_comments['comment'])\n",
    "hot_comments['comment'] = clean_text(hot_comments['comment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4811c6ac",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9b353b",
   "metadata": {},
   "source": [
    "Given that one post can have many comments, I will utilise text summarisation and keywords to extract insights.\n",
    "\n",
    "Our goal is to accumulate all advice into one easily interpretable place. So, if we think of posts like questions, and comments as answers; then a summary of the comments is reasonable advice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05638e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group together comments and scores of the same post\n",
    "summ_top_comments = top_comments.groupby(\"post_id\")['comment'].transform(lambda x: \". \".join(x)) # joins all comments of a post\n",
    "summ_top_comments = pd.concat([top_comments['post_id'], summ_top_comments], axis=1)\n",
    "summ_top_comments_score = top_comments.groupby(\"post_id\")['score'].mean()\n",
    "summ_top_comments = summ_top_comments.merge(summ_top_comments_score, how='inner', on='post_id')\n",
    "summ_top_comments = summ_top_comments.rename(columns={'score': 'avg_comment_score'})\n",
    "summ_top_comments = summ_top_comments.drop_duplicates('post_id').reset_index(drop=True)\n",
    "\n",
    "summ_hot_comments = hot_comments.groupby(\"post_id\")['comment'].transform(lambda x: \". \".join(x))\n",
    "summ_hot_comments = pd.concat([hot_comments['post_id'], summ_hot_comments], axis=1)\n",
    "summ_hot_comments_score = hot_comments.groupby(\"post_id\")['score'].mean()\n",
    "summ_hot_comments = summ_hot_comments.merge(summ_hot_comments_score, how='inner', on='post_id')\n",
    "summ_hot_comments = summ_hot_comments.rename(columns={'score': 'avg_comment_score'})\n",
    "summ_hot_comments = summ_hot_comments.drop_duplicates('post_id').reset_index(drop=True)\n",
    "\n",
    "# join to top/hot dataframe\n",
    "top = top.merge(summ_top_comments, how='inner', left_on='id', right_on='post_id')\n",
    "top = top.drop(columns='post_id')\n",
    "hot = hot.merge(summ_hot_comments, how='inner', left_on='id', right_on='post_id')\n",
    "hot = hot.drop(columns='post_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ec24b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace double fullstops with one\n",
    "top['comment'] = [re.sub(r\"\\.\\.\", \".\", comm) for comm in top['comment']]\n",
    "hot['comment'] = [re.sub(r\"\\.\\.\", \".\", comm) for comm in hot['comment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a7b5b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# round avg_comment_score\n",
    "top['avg_comment_score'] = top['avg_comment_score'].round()\n",
    "hot['avg_comment_score'] = hot['avg_comment_score'].round()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cf97f7",
   "metadata": {},
   "source": [
    "### Keyword Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2cbba0",
   "metadata": {},
   "source": [
    "Extracting the top N keywords from the title, description, or comments will help see what topics are among discussion.\n",
    "\n",
    "By default, we are extracting the top 5 keywords where a 'keyword' can be up to a legth of 3 (3-gram)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3d4bd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get N keywords from given string\n",
    "def get_keywords(text, n_gram_max=3, dup_limit=0.5, max_num_kw=5):\n",
    "    '''\n",
    "    n_gram_max := max size of n-gram (consecutive N-words)\n",
    "    dup_limit := tolerance of duplicate keywords (0.1 = avoid repetition, 0.9 = allow repetition)\n",
    "    max_num_kw := max number of keywords returned\n",
    "    '''\n",
    "    if text == \"\":\n",
    "        return \"\"\n",
    "    else:\n",
    "        custom_kw_extractor = yake.KeywordExtractor(\n",
    "                lan='en',\n",
    "                n=n_gram_max,\n",
    "                dedupLim=dup_limit,\n",
    "                top=max_num_kw,\n",
    "                features=None\n",
    "            )\n",
    "        kw = custom_kw_extractor.extract_keywords(text)\n",
    "        kw = [(tup[1], tup[0]) for tup in kw]\n",
    "        heapq.heapify(kw) # sort by probability\n",
    "        top_kw = heapq.nlargest(max_num_kw, kw) # top-10 keywords\n",
    "        return \". \".join([tup[1] for tup in top_kw])\n",
    "\n",
    "# NOTE: DEFAULT IS 3-GRAM, BALANCED DUPLICATES (0.5), MAX_NUM_KW=5 [to pass other args: .apply(get_keywords, max_num_kw=10, ...)]\n",
    "top['title_kw'] = top['title'].apply(get_keywords)\n",
    "hot['title_kw'] = top['title'].apply(get_keywords)\n",
    "top['description_kw'] = top['description'].apply(get_keywords)\n",
    "hot['description_kw'] = top['description'].apply(get_keywords)\n",
    "top['comment_kw'] = top['comment'].apply(get_keywords)\n",
    "hot['comment_kw'] = top['comment'].apply(get_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a8cd05",
   "metadata": {},
   "source": [
    "### (Extractive) Text Summary\n",
    "We utilise EXTRACTIVE text summary. Words are given probabilities based on how often they occur (frequency). We then use these probabilities to measure how important a sentence (collection of words) is, and extract the 5 most important ones (by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ada021d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_summary(text, topN=5):\n",
    "    if text == \"\":\n",
    "        return \"\"\n",
    "    else:\n",
    "        # tokenise words\n",
    "        words = []\n",
    "        for word in text.split():\n",
    "            word = word.strip(\".\") # remove fullstop\n",
    "            word = word.strip() # remove possible whitespace\n",
    "            words.append(word)\n",
    "        # get word frequencies\n",
    "        word_freq = Counter(words)\n",
    "        # get word probabilties\n",
    "        max_freq = max(word_freq.values())\n",
    "        for word in word_freq:\n",
    "            word_freq[word] = word_freq[word] / max_freq\n",
    "        # get sentence probaility/score\n",
    "        sent_probs = {}\n",
    "        sentences = [sent.strip() for sent in text.split(\".\")]\n",
    "        for sent in sentences:\n",
    "            for w in sent.split():\n",
    "                if w in word_freq:\n",
    "                    if sent not in sent_probs:\n",
    "                        sent_probs[sent] = word_freq[w]\n",
    "                    else:\n",
    "                        sent_probs[sent] += word_freq[w]\n",
    "        # select N most likely sentences (i.e. summarise)\n",
    "        if len(sentences) < topN:\n",
    "            return \". \".join(sentences)\n",
    "        else:\n",
    "            h = [(score, sent) for sent, score in sent_probs.items()]\n",
    "            heapq.heapify(h)\n",
    "            summary = [tup[1] for tup in heapq.nlargest(topN, h)]\n",
    "            return \". \".join(summary)\n",
    "\n",
    "topN = 5 # get top 5 sentences\n",
    "top['description_summ'] = top['description'].apply(get_text_summary, topN=topN)\n",
    "hot['description_summ'] = top['description'].apply(get_text_summary, topN=topN)\n",
    "top['comment_summ'] = top['comment'].apply(get_text_summary, topN=topN)\n",
    "hot['comment_summ'] = top['comment'].apply(get_text_summary, topN=topN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c372bab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author_name</th>\n",
       "      <th>datetime</th>\n",
       "      <th>flair</th>\n",
       "      <th>score</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>comment</th>\n",
       "      <th>avg_comment_score</th>\n",
       "      <th>title_kw</th>\n",
       "      <th>description_kw</th>\n",
       "      <th>comment_kw</th>\n",
       "      <th>description_summ</th>\n",
       "      <th>comment_summ</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataanalysis</td>\n",
       "      <td>wfgn7j</td>\n",
       "      <td>2 month 150 resume 6 interview finally signed ...</td>\n",
       "      <td>SomeEmotion3</td>\n",
       "      <td>2022-08-03</td>\n",
       "      <td>Career Advice</td>\n",
       "      <td>469</td>\n",
       "      <td>0.99</td>\n",
       "      <td>job data analyst well known analytics conpany....</td>\n",
       "      <td>https://www.reddit.com/r/dataanalysis/comments...</td>\n",
       "      <td>stopping say congrats. hey show link project r...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>month. signed job offer. finally signed job. j...</td>\n",
       "      <td>data. job data. conpany. job data analyst. ana...</td>\n",
       "      <td>congratulation. congrats. stopping. project. r...</td>\n",
       "      <td>way type resume way deliver interview matter 6...</td>\n",
       "      <td>wow congratulation happy deserve get better in...</td>\n",
       "      <td>top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataanalysis</td>\n",
       "      <td>10onhl2</td>\n",
       "      <td>want become analyst start</td>\n",
       "      <td>milwted</td>\n",
       "      <td>2023-01-30</td>\n",
       "      <td>Career Advice</td>\n",
       "      <td>459</td>\n",
       "      <td>0.99</td>\n",
       "      <td>starting career data analytics open many excit...</td>\n",
       "      <td>https://www.reddit.com/r/dataanalysis/comments...</td>\n",
       "      <td>great post. upvoted. maybe murphyslab sir quac...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>start. analyst start</td>\n",
       "      <td>experience. data analytics. exciting opportuni...</td>\n",
       "      <td>great. excel. data. start. great post. upvoted</td>\n",
       "      <td>prepared application process like 100 job appl...</td>\n",
       "      <td>another thing help sub weekly stickied enterin...</td>\n",
       "      <td>top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataanalysis</td>\n",
       "      <td>unoys0</td>\n",
       "      <td>google apprenticeship response google 2022</td>\n",
       "      <td>Danielle-Dee</td>\n",
       "      <td>2022-05-12</td>\n",
       "      <td></td>\n",
       "      <td>444</td>\n",
       "      <td>0.99</td>\n",
       "      <td>applied google apprenticeship want anything ne...</td>\n",
       "      <td>https://www.reddit.com/r/dataanalysis/comments...</td>\n",
       "      <td>still waiting take heavy grain salt swe friend...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>apprenticeship. google. response google. googl...</td>\n",
       "      <td>people. bet lot applied. lot applied nervous. ...</td>\n",
       "      <td>friend google close. salt swe friend. grain sa...</td>\n",
       "      <td>x200b update 8 23 wow almost 2 month since con...</td>\n",
       "      <td>8mins ago sent interview ux design program ext...</td>\n",
       "      <td>top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataanalysis</td>\n",
       "      <td>xg0c4u</td>\n",
       "      <td>started google data analytics course july 26th...</td>\n",
       "      <td>GoobGoobb</td>\n",
       "      <td>2022-09-16</td>\n",
       "      <td>Career Advice</td>\n",
       "      <td>355</td>\n",
       "      <td>0.98</td>\n",
       "      <td>basically got lucky. finished course august 27...</td>\n",
       "      <td>https://www.reddit.com/r/dataanalysis/comments...</td>\n",
       "      <td>congrats job offer learned sql intermediate le...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>started. july. analytics course july. google d...</td>\n",
       "      <td>job. lucky. basically. resume. basically got l...</td>\n",
       "      <td>congrats. learned sql intermediate. level star...</td>\n",
       "      <td>nailed interview process signed offer yesterda...</td>\n",
       "      <td>able actually switch career data analytics one...</td>\n",
       "      <td>top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataanalysis</td>\n",
       "      <td>q37irg</td>\n",
       "      <td>google data analysis course review</td>\n",
       "      <td>Free_Dimension1459</td>\n",
       "      <td>2021-10-07</td>\n",
       "      <td></td>\n",
       "      <td>298</td>\n",
       "      <td>1.00</td>\n",
       "      <td>hi week 4 7th course little bit r capstone go ...</td>\n",
       "      <td>https://www.reddit.com/r/dataanalysis/comments...</td>\n",
       "      <td>recently completed full course except capstone...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>data. review. google. analysis course review. ...</td>\n",
       "      <td>prep. skill. interview. data. job</td>\n",
       "      <td>position. job. data. data analysis. recently c...</td>\n",
       "      <td>break 3 category foundation course 1 2 dash th...</td>\n",
       "      <td>data analythics career locked unless go back s...</td>\n",
       "      <td>top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dataanalysis</td>\n",
       "      <td>10ylebf</td>\n",
       "      <td>become advanced data analyst carreer path data...</td>\n",
       "      <td>Gheron</td>\n",
       "      <td>2023-02-10</td>\n",
       "      <td></td>\n",
       "      <td>36</td>\n",
       "      <td>0.93</td>\n",
       "      <td>data analyst two year wondering develop data a...</td>\n",
       "      <td>https://www.reddit.com/r/dataanalysis/comments...</td>\n",
       "      <td>would learn statistic probability hypothesis b...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>career. shift google cert. google cert success...</td>\n",
       "      <td>lot. working. excited share accepted. share ac...</td>\n",
       "      <td>google. story. success. congratulation. congrats</td>\n",
       "      <td>scoured open source dataset website found thin...</td>\n",
       "      <td>damn inspiring considering similar position wo...</td>\n",
       "      <td>hot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dataanalysis</td>\n",
       "      <td>10xz7wz</td>\n",
       "      <td>day data analyst</td>\n",
       "      <td>Immighthaveloat10k</td>\n",
       "      <td>2023-02-09</td>\n",
       "      <td></td>\n",
       "      <td>45</td>\n",
       "      <td>0.92</td>\n",
       "      <td>start professional career data analytics come ...</td>\n",
       "      <td>https://www.reddit.com/r/dataanalysis/comments...</td>\n",
       "      <td>30 meeting 60 cleaning data 10 creating viz pr...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>starting thread. sharing answer. sql excel dat...</td>\n",
       "      <td>resume. wondering getting interviews. analyst....</td>\n",
       "      <td>analyst. excel. year ride started. data. sql</td>\n",
       "      <td>good resume much le important skill listed ess...</td>\n",
       "      <td>bi analyst knowing sql important know bi tool ...</td>\n",
       "      <td>hot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>dataanalysis</td>\n",
       "      <td>10wieqb</td>\n",
       "      <td>first month working data analyst bootcamp. ask...</td>\n",
       "      <td>Think_Thought4982</td>\n",
       "      <td>2023-02-08</td>\n",
       "      <td></td>\n",
       "      <td>88</td>\n",
       "      <td>0.92</td>\n",
       "      <td>food server 20 year left pursue career tech. f...</td>\n",
       "      <td>https://www.reddit.com/r/dataanalysis/comments...</td>\n",
       "      <td>many application submit many interview partici...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>accepted</td>\n",
       "      <td>recently accepted data. dear everyone pleased....</td>\n",
       "      <td>signing nda selection. requires entry exam. nd...</td>\n",
       "      <td>combining effort web development data analytic...</td>\n",
       "      <td>congrats huge step admit never heard data anal...</td>\n",
       "      <td>hot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dataanalysis</td>\n",
       "      <td>10w2i9f</td>\n",
       "      <td>mean strong excel</td>\n",
       "      <td>shmoe94</td>\n",
       "      <td>2023-02-07</td>\n",
       "      <td>Data Tools</td>\n",
       "      <td>44</td>\n",
       "      <td>0.91</td>\n",
       "      <td>one understand comfortable considered strong e...</td>\n",
       "      <td>https://www.reddit.com/r/dataanalysis/comments...</td>\n",
       "      <td>know power query pivot table index match proba...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>awesome. offer. interview. application. awesom...</td>\n",
       "      <td>encouragement like faced. transitioning data a...</td>\n",
       "      <td>congrats. career. started learn data. learn da...</td>\n",
       "      <td>wanted post encouragement like faced discourag...</td>\n",
       "      <td>congratulation feel coursera certificate helpe...</td>\n",
       "      <td>hot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dataanalysis</td>\n",
       "      <td>10vcq67</td>\n",
       "      <td>subreddit feature request would cool able choo...</td>\n",
       "      <td>datagorb</td>\n",
       "      <td>2023-02-06</td>\n",
       "      <td>Career Advice</td>\n",
       "      <td>54</td>\n",
       "      <td>0.95</td>\n",
       "      <td>think could really helpful ways.</td>\n",
       "      <td>https://www.reddit.com/r/dataanalysis/comments...</td>\n",
       "      <td>please make sure stayed holiday inn express av...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>program. finished. google data analytics. data...</td>\n",
       "      <td>professional certificate program. share. excit...</td>\n",
       "      <td>ibm. congrats. great. working ibm. congratulation</td>\n",
       "      <td>wanted share someone. excited completed google...</td>\n",
       "      <td>still working mine glad see someone else simil...</td>\n",
       "      <td>hot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>370 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       subreddit       id                                              title  \\\n",
       "0   dataanalysis   wfgn7j  2 month 150 resume 6 interview finally signed ...   \n",
       "1   dataanalysis  10onhl2                          want become analyst start   \n",
       "2   dataanalysis   unoys0         google apprenticeship response google 2022   \n",
       "3   dataanalysis   xg0c4u  started google data analytics course july 26th...   \n",
       "4   dataanalysis   q37irg                 google data analysis course review   \n",
       "..           ...      ...                                                ...   \n",
       "11  dataanalysis  10ylebf  become advanced data analyst carreer path data...   \n",
       "12  dataanalysis  10xz7wz                                   day data analyst   \n",
       "13  dataanalysis  10wieqb  first month working data analyst bootcamp. ask...   \n",
       "14  dataanalysis  10w2i9f                                  mean strong excel   \n",
       "15  dataanalysis  10vcq67  subreddit feature request would cool able choo...   \n",
       "\n",
       "           author_name    datetime          flair  score  upvote_ratio  \\\n",
       "0         SomeEmotion3  2022-08-03  Career Advice    469          0.99   \n",
       "1              milwted  2023-01-30  Career Advice    459          0.99   \n",
       "2         Danielle-Dee  2022-05-12                   444          0.99   \n",
       "3            GoobGoobb  2022-09-16  Career Advice    355          0.98   \n",
       "4   Free_Dimension1459  2021-10-07                   298          1.00   \n",
       "..                 ...         ...            ...    ...           ...   \n",
       "11              Gheron  2023-02-10                    36          0.93   \n",
       "12  Immighthaveloat10k  2023-02-09                    45          0.92   \n",
       "13   Think_Thought4982  2023-02-08                    88          0.92   \n",
       "14             shmoe94  2023-02-07     Data Tools     44          0.91   \n",
       "15            datagorb  2023-02-06  Career Advice     54          0.95   \n",
       "\n",
       "                                          description  \\\n",
       "0   job data analyst well known analytics conpany....   \n",
       "1   starting career data analytics open many excit...   \n",
       "2   applied google apprenticeship want anything ne...   \n",
       "3   basically got lucky. finished course august 27...   \n",
       "4   hi week 4 7th course little bit r capstone go ...   \n",
       "..                                                ...   \n",
       "11  data analyst two year wondering develop data a...   \n",
       "12  start professional career data analytics come ...   \n",
       "13  food server 20 year left pursue career tech. f...   \n",
       "14  one understand comfortable considered strong e...   \n",
       "15                   think could really helpful ways.   \n",
       "\n",
       "                                                  url  \\\n",
       "0   https://www.reddit.com/r/dataanalysis/comments...   \n",
       "1   https://www.reddit.com/r/dataanalysis/comments...   \n",
       "2   https://www.reddit.com/r/dataanalysis/comments...   \n",
       "3   https://www.reddit.com/r/dataanalysis/comments...   \n",
       "4   https://www.reddit.com/r/dataanalysis/comments...   \n",
       "..                                                ...   \n",
       "11  https://www.reddit.com/r/dataanalysis/comments...   \n",
       "12  https://www.reddit.com/r/dataanalysis/comments...   \n",
       "13  https://www.reddit.com/r/dataanalysis/comments...   \n",
       "14  https://www.reddit.com/r/dataanalysis/comments...   \n",
       "15  https://www.reddit.com/r/dataanalysis/comments...   \n",
       "\n",
       "                                              comment  avg_comment_score  \\\n",
       "0   stopping say congrats. hey show link project r...               23.0   \n",
       "1   great post. upvoted. maybe murphyslab sir quac...               11.0   \n",
       "2   still waiting take heavy grain salt swe friend...               44.0   \n",
       "3   congrats job offer learned sql intermediate le...               18.0   \n",
       "4   recently completed full course except capstone...                9.0   \n",
       "..                                                ...                ...   \n",
       "11  would learn statistic probability hypothesis b...                8.0   \n",
       "12  30 meeting 60 cleaning data 10 creating viz pr...               28.0   \n",
       "13  many application submit many interview partici...               13.0   \n",
       "14  know power query pivot table index match proba...               14.0   \n",
       "15  please make sure stayed holiday inn express av...                8.0   \n",
       "\n",
       "                                             title_kw  \\\n",
       "0   month. signed job offer. finally signed job. j...   \n",
       "1                                start. analyst start   \n",
       "2   apprenticeship. google. response google. googl...   \n",
       "3   started. july. analytics course july. google d...   \n",
       "4   data. review. google. analysis course review. ...   \n",
       "..                                                ...   \n",
       "11  career. shift google cert. google cert success...   \n",
       "12  starting thread. sharing answer. sql excel dat...   \n",
       "13                                           accepted   \n",
       "14  awesome. offer. interview. application. awesom...   \n",
       "15  program. finished. google data analytics. data...   \n",
       "\n",
       "                                       description_kw  \\\n",
       "0   data. job data. conpany. job data analyst. ana...   \n",
       "1   experience. data analytics. exciting opportuni...   \n",
       "2   people. bet lot applied. lot applied nervous. ...   \n",
       "3   job. lucky. basically. resume. basically got l...   \n",
       "4                   prep. skill. interview. data. job   \n",
       "..                                                ...   \n",
       "11  lot. working. excited share accepted. share ac...   \n",
       "12  resume. wondering getting interviews. analyst....   \n",
       "13  recently accepted data. dear everyone pleased....   \n",
       "14  encouragement like faced. transitioning data a...   \n",
       "15  professional certificate program. share. excit...   \n",
       "\n",
       "                                           comment_kw  \\\n",
       "0   congratulation. congrats. stopping. project. r...   \n",
       "1      great. excel. data. start. great post. upvoted   \n",
       "2   friend google close. salt swe friend. grain sa...   \n",
       "3   congrats. learned sql intermediate. level star...   \n",
       "4   position. job. data. data analysis. recently c...   \n",
       "..                                                ...   \n",
       "11   google. story. success. congratulation. congrats   \n",
       "12       analyst. excel. year ride started. data. sql   \n",
       "13  signing nda selection. requires entry exam. nd...   \n",
       "14  congrats. career. started learn data. learn da...   \n",
       "15  ibm. congrats. great. working ibm. congratulation   \n",
       "\n",
       "                                     description_summ  \\\n",
       "0   way type resume way deliver interview matter 6...   \n",
       "1   prepared application process like 100 job appl...   \n",
       "2   x200b update 8 23 wow almost 2 month since con...   \n",
       "3   nailed interview process signed offer yesterda...   \n",
       "4   break 3 category foundation course 1 2 dash th...   \n",
       "..                                                ...   \n",
       "11  scoured open source dataset website found thin...   \n",
       "12  good resume much le important skill listed ess...   \n",
       "13  combining effort web development data analytic...   \n",
       "14  wanted post encouragement like faced discourag...   \n",
       "15  wanted share someone. excited completed google...   \n",
       "\n",
       "                                         comment_summ origin  \n",
       "0   wow congratulation happy deserve get better in...    top  \n",
       "1   another thing help sub weekly stickied enterin...    top  \n",
       "2   8mins ago sent interview ux design program ext...    top  \n",
       "3   able actually switch career data analytics one...    top  \n",
       "4   data analythics career locked unless go back s...    top  \n",
       "..                                                ...    ...  \n",
       "11  damn inspiring considering similar position wo...    hot  \n",
       "12  bi analyst knowing sql important know bi tool ...    hot  \n",
       "13  congrats huge step admit never heard data anal...    hot  \n",
       "14  congratulation feel coursera certificate helpe...    hot  \n",
       "15  still working mine glad see someone else simil...    hot  \n",
       "\n",
       "[370 rows x 18 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge HOT and TOP dataframes\n",
    "top['origin'] = 'top'\n",
    "hot['origin'] = 'hot'\n",
    "data = pd.concat([top, hot], axis=0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc939ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 370 entries, 0 to 15\n",
      "Data columns (total 18 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   subreddit          370 non-null    object \n",
      " 1   id                 370 non-null    object \n",
      " 2   title              370 non-null    object \n",
      " 3   author_name        370 non-null    object \n",
      " 4   datetime           370 non-null    object \n",
      " 5   flair              370 non-null    object \n",
      " 6   score              370 non-null    int64  \n",
      " 7   upvote_ratio       370 non-null    float64\n",
      " 8   description        370 non-null    object \n",
      " 9   url                370 non-null    object \n",
      " 10  comment            370 non-null    object \n",
      " 11  avg_comment_score  370 non-null    float64\n",
      " 12  title_kw           370 non-null    object \n",
      " 13  description_kw     370 non-null    object \n",
      " 14  comment_kw         370 non-null    object \n",
      " 15  description_summ   370 non-null    object \n",
      " 16  comment_summ       370 non-null    object \n",
      " 17  origin             370 non-null    object \n",
      "dtypes: float64(2), int64(1), object(15)\n",
      "memory usage: 54.9+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ba07bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save to file\n",
    "# data.to_csv(\".\\\\data\\\\clean_data_posts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfce4b77",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8931f4d6",
   "metadata": {},
   "source": [
    "Given that we are dealing with sentences, there are not many visualisations we can produce to reflect this. This is fine, since my original goal was to summarise all advice to something more compact/digestable. But, we can create visuals with keywords if needed. \n",
    "\n",
    "Note: Sentiment is not of interest in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443c55f3",
   "metadata": {},
   "source": [
    "Hierarchy:\n",
    "- Subreddit\n",
    "    - Flair | Origin\n",
    "        - Title | Description\n",
    "            - Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d33c61d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup (for data exploration)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None) # full length text\n",
    "pd.set_option('display.max_rows', 500)\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv(\".\\\\data\\\\clean_data_posts.csv\", keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6dddec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions which create widgets\n",
    "def get_dropdown(df, colname):\n",
    "    values = df[colname].unique().tolist()\n",
    "    dropdown = widgets.Dropdown(\n",
    "        options = ['All'] + values,\n",
    "        value = 'All',\n",
    "        description = f\"{colname}:\",\n",
    "        continuous_update=False\n",
    "    )\n",
    "    return dropdown\n",
    "\n",
    "def get_kw_box(df, colname):\n",
    "    textbox = widgets.Text(\n",
    "        value = \"\",\n",
    "        placeholder = \"\",\n",
    "        description = f\"Keyword Search for {colname}:\",\n",
    "        display='flex',\n",
    "        flex_flow='column',\n",
    "        align_items='stretch',\n",
    "        style= {'description_width': 'initial'},\n",
    "        continuous_update=False\n",
    "    )\n",
    "    return textbox\n",
    "\n",
    "def get_columns(df, pat=\"\"): # return dropdown of column names\n",
    "    if pat == \"\":\n",
    "        values = df.columns.tolist()\n",
    "    else:\n",
    "        values = [colname for colname in df.columns if pat in colname]\n",
    "    dropdown = kw_columns_dd = widgets.Dropdown(\n",
    "        options = values,\n",
    "        value = 'title_kw',\n",
    "        description = f\"Keyword Column:\",\n",
    "        display='flex',\n",
    "        flex_flow='column',\n",
    "        align_items='stretch',\n",
    "        style= {'description_width': 'initial'},\n",
    "        continuous_update=False\n",
    "    )\n",
    "    return dropdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee924763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_dd_filter(df, colname, choice):\n",
    "    if choice == \"All\":\n",
    "        return df\n",
    "    else:\n",
    "        return df[df[colname] == choice]\n",
    "\n",
    "def apply_text_filter(df, colname, kw):\n",
    "    if kw == \"\":\n",
    "        return df\n",
    "    else:\n",
    "        df = df.reset_index(drop=True)\n",
    "        kw = kw.lower()\n",
    "        keep_i = []\n",
    "        for i, text in enumerate(df[colname]):\n",
    "            for word in text.split(\".\"):\n",
    "                if word.strip() == kw:\n",
    "                    keep_i.append(i)\n",
    "                    break\n",
    "        return df.iloc[keep_i, :]\n",
    "\n",
    "def display_table(subreddit_opt, flair_opt, origin_opt, title_kw_opt, description_kw_opt, comment_kw_opt):\n",
    "    df = data.copy()\n",
    "    df = apply_dd_filter(df, 'subreddit', subreddit_opt)\n",
    "    df = apply_dd_filter(df, 'flair', flair_opt)\n",
    "    df = apply_dd_filter(df, 'origin', origin_opt)\n",
    "    df = apply_text_filter(df, 'title_kw', title_kw_opt)\n",
    "    df = apply_text_filter(df, 'description_kw', description_kw_opt)\n",
    "    df = apply_text_filter(df, 'comment_kw', comment_kw_opt)\n",
    "    print(\"Note: press ENTER after text input.\")\n",
    "    df = df.sort_values(['upvote_ratio','avg_comment_score'], ascending=False)\n",
    "    comment_kws = [w for string in df['comment_kw'] for w in string.split(\". \")]\n",
    "    df = df[['subreddit','title','datetime','flair','origin','upvote_ratio','description_summ','comment_summ','avg_comment_score']]\n",
    "    return df\n",
    "\n",
    "def display_wc(subreddit_opt, flair_opt, origin_opt, kw_col_opt):\n",
    "    df = data.copy()\n",
    "    df = apply_dd_filter(df, 'subreddit', subreddit_opt)\n",
    "    df = apply_dd_filter(df, 'flair', flair_opt)\n",
    "    df = apply_dd_filter(df, 'origin', origin_opt)\n",
    "    kws = [word for string in df[kw_col_opt] for word in string.split(\". \")]\n",
    "    kws = \" \".join(kws)\n",
    "    wc = WordCloud(width=800, height=600, min_font_size=10, background_color=\"white\").generate(kws)\n",
    "    plt.figure(figsize = (8, 8), facecolor = None)\n",
    "    plt.imshow(wc)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad = 0)\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8133801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Widgets\n",
    "subreddit_dd = get_dropdown(data, 'subreddit')\n",
    "flair_dd = get_dropdown(data, 'flair')\n",
    "origin_dd = get_dropdown(data, 'origin')\n",
    "title_kw_box = get_kw_box(data, 'title_kw')\n",
    "description_kw_box = get_kw_box(data, 'description_kw')\n",
    "comment_kw_box = get_kw_box(data, 'comment_kw')\n",
    "kw_column_dd = get_columns(data, '_kw')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc265761",
   "metadata": {},
   "source": [
    "### Summarised Posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "581cb40e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b549a8ec174e77b142120c92663d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='subreddit:', options=('All', 'dataanalysis', 'datascience'), valueâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display Data Table\n",
    "widgets.interact(\n",
    "    display_table,\n",
    "    subreddit_opt=subreddit_dd,\n",
    "    flair_opt=flair_dd,\n",
    "    origin_opt=origin_dd,\n",
    "    title_kw_opt=title_kw_box,\n",
    "    description_kw_opt=description_kw_box,\n",
    "    comment_kw_opt=comment_kw_box\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc7ccac",
   "metadata": {},
   "source": [
    "Improvements for next time...\n",
    "- Colour the keywords in summarised text using pandas styling (e.g. https://monkeylearn.com/static/3b4b48a512024d2f139ce5324534bf9f/b7203/studio-chewy.webp)\n",
    "- Group similar flairs (since some flairs are specific to certain subreddits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a81efc",
   "metadata": {},
   "source": [
    "### Keywords from Posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6aad0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c25ff3a7253346fbb0f64cb8c21cfc4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='subreddit:', options=('All', 'dataanalysis', 'datascience'), valueâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display word cloud of keywords\n",
    "widgets.interact(\n",
    "    display_wc,\n",
    "    subreddit_opt=subreddit_dd,\n",
    "    flair_opt=flair_dd,\n",
    "    origin_opt=origin_dd,\n",
    "    kw_col_opt=kw_column_dd\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0721122d",
   "metadata": {},
   "source": [
    "Note: if error produced - means that filter results returns empty."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
